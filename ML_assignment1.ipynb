{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8b320e-1fd4-4def-87c6-4ff420d07b47",
   "metadata": {},
   "source": [
    "## Objective of this assignment\n",
    "In this assignment, you will choose two classical machine learning algorithms implemented by\n",
    "scikit-learn, spend some time researching them, then describe each algorithm in your own words,\n",
    "compare and contrast their strengths and weaknesses, and apply both algorithms to the same\n",
    "dataset - of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb6823-781b-446f-9506-fde9d89abc44",
   "metadata": {},
   "source": [
    "## Part 1: Algorithm Selection\n",
    "#### Decision: Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56363-b974-4c67-b3a9-342a68ce3617",
   "metadata": {},
   "source": [
    "## Part 2: Description\n",
    "\n",
    "### Main Concepts\n",
    "**Decision Tree**\n",
    "is a hierarchical model with root, internal, and leaf nodes that make sequential decisions. It recursively partitions data using feature thresholds that maximize information gain. It uses Gini impurity or entropy during training to find the purest splits. For prediction, it follows a single path from root to leaf for final classification. Highly interpretable \"white box\" model that shows clear decision rules.\n",
    "\n",
    "**Random Forest**\n",
    "is a model that combines multiple decision trees to create a stronger predictor. Each tree trains on random data subsets (bootstrap samples) for diversity. As regards Feature Randomness, each split considers only random feature subsets to decorrelate trees. As regards prediction, it adopts simple majority to make its final prediction determined by combining all tree votes. This model reduces overfitting through collective wisdom - errors of individual trees cancel out.\n",
    "\n",
    "#### How the Algorithms Work\n",
    "**Decision Tree** builds a single tree by repeatedly finding the \"best\" feature splits that separate classes most effectively, creating a flowchart-like structure where each path from root to leaf represents a classification rule.\n",
    "\n",
    "**Random Forest** builds hundreds of decision trees, each trained on random data samples and using random feature subsets, then combines their predictions through majority voting to produce a more accurate and stable final result.\n",
    "\n",
    "**Key Difference**: Decision Tree creates one optimized model, while Random Forest creates many diverse models and averages their predictions to reduce errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ceecf-3f80-4bd3-8ae9-3d0db786c242",
   "metadata": {},
   "source": [
    "### Part 3: Comparison\n",
    "\n",
    "## Decision Tree Strengths\n",
    "**High Interpretability**: Easy to understand and explain - shows clear decision rules\n",
    "\n",
    "**No Data Preprocessing**: Handles both numerical and categorical data without scaling\n",
    "\n",
    "**Fast Prediction**: Makes quick decisions by following a simple path\n",
    "\n",
    "**Feature Importance**: Naturally ranks feature importance through split selection\n",
    "\n",
    "**Handles Non-linearity**: Captures complex relationships without transformation\n",
    "\n",
    "## Random Forest Strengths\n",
    "**High Accuracy**: Typically outperforms single decision trees and many other algorithms\n",
    "\n",
    "**Reduced Overfitting**: Averaging multiple trees prevents overfitting to noise\n",
    "\n",
    "**Handles Missing Data**: Robust to missing values through bootstrap sampling\n",
    "\n",
    "**Feature Importance**: Provides more reliable importance scores than single trees\n",
    "\n",
    "**Works \"Out-of-the-Box\"**: Requires less parameter tuning than many algorithms\n",
    "\n",
    "**Parallelizable**: Trees can be built simultaneously for faster training\n",
    "\n",
    "## Decision Tree Weaknesses\n",
    "\n",
    "**Prone to Overfitting**: Can create overly complex trees that memorize noise rather than learning patterns\n",
    "\n",
    "**High Variance**: Small data changes can lead to completely different tree structures\n",
    "\n",
    "**Unstable**: Sensitive to training data variations - poor generalization\n",
    "\n",
    "**Greedy Nature**: Makes locally optimal decisions that may not be globally optimal\n",
    "\n",
    "**Limited Performance**: Often outperformed by ensemble methods on complex tasks\n",
    "\n",
    "**Biased with Imbalanced Data**: Can create skewed trees if some classes dominate\n",
    "\n",
    "## Random Forest Weaknesses\n",
    "\n",
    "**Black Box Model**: Difficult to interpret - loses Decision Tree's transparency\n",
    "\n",
    "**Computationally Expensive**: Requires more memory and processing power\n",
    "\n",
    "**Slower Prediction**: Must run through multiple trees instead of one\n",
    "\n",
    "**Overfitting Risk**: Can still overfit on very noisy datasets\n",
    "\n",
    "**Memory Intensive**: Stores multiple large trees in memory\n",
    "\n",
    "**Parameter Sensitivity**: Performance depends on proper tuning of tree count and depth\n",
    "\n",
    "## Core Strength of algorithms\n",
    "**Decision Tree**: Best when you need model transparency and explanation\n",
    "\n",
    "**Random Forest**: Best when you need maximum predictive accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48908f32-88bc-409f-826d-ae6469e568ea",
   "metadata": {},
   "source": [
    "## Part 4: Application of Dataset\n",
    "The dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a8748f-e650-471a-baec-28880b788f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
