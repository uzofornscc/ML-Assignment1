{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8b320e-1fd4-4def-87c6-4ff420d07b47",
   "metadata": {},
   "source": [
    "## Objective of this assignment\n",
    "In this assignment, you will choose two classical machine learning algorithms implemented by\n",
    "scikit-learn, spend some time researching them, then describe each algorithm in your own words,\n",
    "compare and contrast their strengths and weaknesses, and apply both algorithms to the same\n",
    "dataset - of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb6823-781b-446f-9506-fde9d89abc44",
   "metadata": {},
   "source": [
    "## Part 1: Algorithm Selection\n",
    "#### I will be looking at Logistic Regression and Support Vector Machine models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56363-b974-4c67-b3a9-342a68ce3617",
   "metadata": {},
   "source": [
    "## Part 2: Description\n",
    "\n",
    "### Main Concepts\n",
    "**Logistic Regression Model**\n",
    "1. Predict a binary outcome (Yes/No, 0/1) by modeling the probability that a data point belongs to a class or not.\n",
    "2. Uses a linear combination of features, then squashes that value through the logistic (sigmoid) function to get a probability between 0 and 1.\n",
    "\n",
    "**Support Vector Machine (SVM) model**\n",
    "1. Finds a decision boundary (a hyperplane) that separates classes with the maximum margin (distance between closest points of each class to the boundary).\n",
    "2. Can create non-linear boundaries using kernels (e.g., RBF) by implicitly mapping data to higher dimensions.\n",
    "\n",
    "#### How the Algorithms Work\n",
    "**Logistic Regression**\n",
    "1. Taking a linear combination of the features.\n",
    "2. Turning it into a probability using the sigmoid function.\n",
    "3. Comparing predictions to real outcomes.\n",
    "4. Adjusting weights to minimize errors.\n",
    "5. Repeating until the model learns the best boundary between classes.\n",
    "\n",
    "**Support vector Machine**\n",
    "1. Finding the best separating line (or surface) between classes.\n",
    "2. Maximizing the distance between that line and the closest points (support vectors).\n",
    "3. Using kernel tricks to handle non-linear data.\n",
    "4. Controlling flexibility with the regularization parameter (C).\n",
    "5. Predicting class based on which side of the boundary a new point lies.\n",
    "\n",
    "#### Part 3: Comparison and Key Differences: Logistic Regression vs SVM\n",
    "\n",
    "| **Aspect**                         | **Logistic Regression (LR)**                                                      | **Support Vector Machine (SVM)**                                                             |\n",
    "| ---------------------------------- | --------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| **Type of Model**                  | Probabilistic (outputs probabilities)                                             | Geometric / Margin-based (finds best boundary)                                               |\n",
    "| **Main Idea**                      | Finds a linear boundary that best fits data by minimizing the log loss.           | Finds the hyperplane that maximizes the margin between classes.                              |\n",
    "| **Output**                         | Predicts probabilities (e.g., “70% chance of class 1”).                           | Predicts class labels directly (can give probability if enabled).                            |\n",
    "| **Decision Boundary**              | Based on the sigmoid curve — smooth and probabilistic.                            | Based on support vectors — sharp margin separation.                                          |\n",
    "| **Handling Non-linearity**         | Needs feature engineering (like polynomial features) to handle non-linear data.   | Uses kernel functions (RBF, polynomial) to model complex, non-linear patterns.               |\n",
    "| **Regularization Parameter**       | Controlled by `C` (inverse of regularization strength).                           | Also controlled by `C` (trade-off between margin width and misclassification).               |\n",
    "| **Interpretability**               | <font color=\"green\"> Easy to interpret — coefficients show feature influence.                          | Harder to interpret, especially with non-linear kernels.  </font>                                   |\n",
    "| **Computation Speed**              | <font color=\"green\">Faster on large datasets.</font>                                                         | Slower on large datasets (especially with non-linear kernels).                               |\n",
    "| **Sensitivity to Feature Scaling** | Not highly sensitive but scaling helps.                                           | Highly sensitive — scaling is essential.                                                     |\n",
    "| **Typical Use Cases**              | When interpretability and probabilities matter (e.g., credit scoring, marketing). | <font color=\"green\">When accuracy and clear separation matter (e.g., image classification, text classification).<font> |\n",
    "\n",
    "<font color='green'>_green indicates strength. which is a weakness for the other_</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48908f32-88bc-409f-826d-ae6469e568ea",
   "metadata": {},
   "source": [
    "## Part 4: Application of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73eb14-8ac4-4c1c-b9ce-a895827ad80a",
   "metadata": {},
   "source": [
    "### Developing two independent models on social network dataset found on Kaggle\n",
    "\n",
    "[Social Network Ads @ Kaggle](https://www.kaggle.com/datasets/d4rklucif3r/social-network-ads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c4878-658e-4be1-9c38-72c2d91248c8",
   "metadata": {},
   "source": [
    "##### Objective: The Dataset used in these models tells about whether a person of certain age having certain income purchases a product or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa34f59-ba61-4655-8288-ca2598081ced",
   "metadata": {},
   "source": [
    "#### ❓ Questions I Seek to answer\n",
    "\n",
    "1. Did the user purchase ads or not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a7fe8-e976-4cc7-bfe4-13870abe124d",
   "metadata": {},
   "source": [
    "The modelling process will be separated from this notebook for the sake of separation of concerns. \n",
    "\n",
    "**Here is the link to both ML models used on this dataset:**\n",
    "[Logistic Regression and SVM model](social_ntwrk_model.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
