{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8b320e-1fd4-4def-87c6-4ff420d07b47",
   "metadata": {},
   "source": [
    "## Objective of this assignment\n",
    "In this assignment, you will choose two classical machine learning algorithms implemented by\n",
    "scikit-learn, spend some time researching them, then describe each algorithm in your own words,\n",
    "compare and contrast their strengths and weaknesses, and apply both algorithms to the same\n",
    "dataset - of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb6823-781b-446f-9506-fde9d89abc44",
   "metadata": {},
   "source": [
    "## Part 1: Algorithm Selection\n",
    "#### Decision: Decision Tree and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e56363-b974-4c67-b3a9-342a68ce3617",
   "metadata": {},
   "source": [
    "## Part 2: Description\n",
    "\n",
    "### Main Concepts\n",
    "**Decision Tree**\n",
    "is a hierarchical model with root, internal, and leaf nodes that make sequential decisions. It recursively partitions data using feature thresholds that maximize information gain. It uses Gini impurity or entropy during training to find the purest splits. For prediction, it follows a single path from root to leaf for final classification. Highly interpretable \"white box\" model that shows clear decision rules.\n",
    "\n",
    "**Random Forest**\n",
    "is a model that combines multiple decision trees to create a stronger predictor. Each tree trains on random data subsets (bootstrap samples) for diversity. As regards Feature Randomness, each split considers only random feature subsets to decorrelate trees. As regards prediction, it adopts simple majority to make its final prediction determined by combining all tree votes. This model reduces overfitting through collective wisdom - errors of individual trees cancel out.\n",
    "\n",
    "#### How the Algorithms Work\n",
    "**Decision Tree** builds a single tree by repeatedly finding the \"best\" feature splits that separate classes most effectively, creating a flowchart-like structure where each path from root to leaf represents a classification rule.\n",
    "\n",
    "**Random Forest** builds hundreds of decision trees, each trained on random data samples and using random feature subsets, then combines their predictions through majority voting to produce a more accurate and stable final result.\n",
    "\n",
    "**Key Difference**: Decision Tree creates one optimized model, while Random Forest creates many diverse models and averages their predictions to reduce errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ceecf-3f80-4bd3-8ae9-3d0db786c242",
   "metadata": {},
   "source": [
    "### Part 3: Comparison\n",
    "\n",
    "## Decision Tree Strengths\n",
    "**High Interpretability**: Easy to understand and explain - shows clear decision rules\n",
    "\n",
    "**No Data Preprocessing**: Handles both numerical and categorical data without scaling\n",
    "\n",
    "**Fast Prediction**: Makes quick decisions by following a simple path\n",
    "\n",
    "**Feature Importance**: Naturally ranks feature importance through split selection\n",
    "\n",
    "**Handles Non-linearity**: Captures complex relationships without transformation\n",
    "\n",
    "## Random Forest Strengths\n",
    "**High Accuracy**: Typically outperforms single decision trees and many other algorithms\n",
    "\n",
    "**Reduced Overfitting**: Averaging multiple trees prevents overfitting to noise\n",
    "\n",
    "**Handles Missing Data**: Robust to missing values through bootstrap sampling\n",
    "\n",
    "**Feature Importance**: Provides more reliable importance scores than single trees\n",
    "\n",
    "**Works \"Out-of-the-Box\"**: Requires less parameter tuning than many algorithms\n",
    "\n",
    "**Parallelizable**: Trees can be built simultaneously for faster training\n",
    "\n",
    "## Decision Tree Weaknesses\n",
    "\n",
    "**Prone to Overfitting**: Can create overly complex trees that memorize noise rather than learning patterns\n",
    "\n",
    "**High Variance**: Small data changes can lead to completely different tree structures\n",
    "\n",
    "**Unstable**: Sensitive to training data variations - poor generalization\n",
    "\n",
    "**Greedy Nature**: Makes locally optimal decisions that may not be globally optimal\n",
    "\n",
    "**Limited Performance**: Often outperformed by ensemble methods on complex tasks\n",
    "\n",
    "**Biased with Imbalanced Data**: Can create skewed trees if some classes dominate\n",
    "\n",
    "## Random Forest Weaknesses\n",
    "\n",
    "**Black Box Model**: Difficult to interpret - loses Decision Tree's transparency\n",
    "\n",
    "**Computationally Expensive**: Requires more memory and processing power\n",
    "\n",
    "**Slower Prediction**: Must run through multiple trees instead of one\n",
    "\n",
    "**Overfitting Risk**: Can still overfit on very noisy datasets\n",
    "\n",
    "**Memory Intensive**: Stores multiple large trees in memory\n",
    "\n",
    "**Parameter Sensitivity**: Performance depends on proper tuning of tree count and depth\n",
    "\n",
    "## Core Strength of algorithms\n",
    "**Decision Tree**: Best when you need model transparency and explanation\n",
    "\n",
    "**Random Forest**: Best when you need maximum predictive accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48908f32-88bc-409f-826d-ae6469e568ea",
   "metadata": {},
   "source": [
    "## Part 4: Application of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73eb14-8ac4-4c1c-b9ce-a895827ad80a",
   "metadata": {},
   "source": [
    "### Predicting Sleep Trouble Among Older Adults\n",
    "#### A Machine Learning Analysis Using the National Poll on Healthy Aging (NPHA)\n",
    "\n",
    "[Malani, P. N., Kullgren, J., & Solway, E. (2019). National Poll on Healthy Aging (NPHA), [United States], April 2017 (ICPSR 37305) [Data set]. Inter-university Consortium for Political and Social Research (ICPSR).]( https://doi.org/10.3886/ICPSR37305.v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c4878-658e-4be1-9c38-72c2d91248c8",
   "metadata": {},
   "source": [
    "##### Objective: The purpose of this project is to use machine learning models (via scikit-learn) to predict whether a patient reports trouble sleeping based on demographic, health, and lifestyle factors collected in the National Poll on Healthy Aging (NPHA).\n",
    "\n",
    "The study aims to:\n",
    "1. Identify which health and lifestyle factors are most predictive of sleep trouble.\n",
    "2. Evaluate how well a supervised learning model (**Logistic Regression, Random Forest**) can classify individuals into ‚Äúhas trouble sleeping‚Äù vs ‚Äúdoes not have trouble sleeping‚Äù.\n",
    "3. Provide interpretable insights that may inform healthcare or behavioral interventions for older adults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa34f59-ba61-4655-8288-ca2598081ced",
   "metadata": {},
   "source": [
    "#### ‚ùì Questions I Seek to answer\n",
    "\n",
    "1. Which variables (e.g., pain, medication, stress, physical or mental health) most strongly predict sleep trouble?\n",
    "2. Can machine learning models **reliably** predict sleep trouble using survey-based data?\n",
    "3. How do demographic factors (age, race, gender, employment) interact with health indicators in shaping sleep health?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc08082-3bd5-4d56-8867-f4ea8e3d0438",
   "metadata": {},
   "source": [
    "#### üìã Dataset Description\n",
    "\n",
    "The NPHA dataset contains survey responses from U.S. adults aged 50‚Äì80 about:\n",
    "\n",
    "1. Physical, mental, and dental health\n",
    "\n",
    "2. Sleep behaviors and challenges\n",
    "\n",
    "3. Use of medications\n",
    "\n",
    "4. Doctor visits and healthcare engagement\n",
    "\n",
    "5. Demographics (age, gender, race, employment)\n",
    "\n",
    "For this analysis, the following subset of variables will be used:\n",
    "\n",
    "| Variable                           | Description                          | Type        | Role                |\n",
    "| ---------------------------------- | ------------------------------------ | ----------- | ------------------- |\n",
    "| Number of Doctors Visited          | Number of different doctors seen     | Ordinal     | Feature             |\n",
    "| Age                                | Patient‚Äôs age group (50‚Äì64, 65‚Äì80)   | Categorical | Feature             |\n",
    "| Physical Health                    | Self-reported physical health rating | Ordinal     | Feature             |\n",
    "| Mental Health                      | Self-reported mental health rating   | Ordinal     | Feature             |\n",
    "| Dental Health                      | Self-reported dental health rating   | Ordinal     | Feature             |\n",
    "| Employment                         | Employment or work status            | Categorical | Feature             |\n",
    "| Stress Keeps from Sleeping         | Stress prevents sleep                | Binary      | Feature             |\n",
    "| Medication Keeps from Sleeping     | Medication prevents sleep            | Binary      | Feature             |\n",
    "| Pain Keeps from Sleeping           | Pain prevents sleep                  | Binary      | Feature             |\n",
    "| Bathroom Needs Keeps from Sleeping | Bathroom needs prevent sleep         | Binary      | Feature             |\n",
    "| Unknown Keeps from Sleeping        | Other unknown causes prevent sleep   | Binary      | Feature             |\n",
    "| Prescription Sleep Medication      | Use of prescription sleep medication | Categorical | Feature             |\n",
    "| Race                               | Racial/ethnic background             | Categorical | Feature             |\n",
    "| Gender                             | Gender identity                      | Categorical | Feature             |\n",
    "| **Trouble Sleeping**               | Reports having trouble sleeping      | Binary      | **Target Variable** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218a7fe8-e976-4cc7-bfe4-13870abe124d",
   "metadata": {},
   "source": [
    "[Applying this dataset using Logistic Regression model](logistic.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18e7c5-f993-40ac-9aa1-1098e700e7da",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
